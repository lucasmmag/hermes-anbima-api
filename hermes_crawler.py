import requests
from bs4 import BeautifulSoup
import os
import re
from urllib.parse import urljoin

API_DOCUMENTOS_URL = "https://bolder-hot-hockey.glitch.me/anbima/documentos"
BASE_SITE_ANBIMA = "https://www.anbima.com.br"
PASTA_SAIDA = "documentos"

# Lista ampliada de palavras-chave relacionadas √† regula√ß√£o
PALAVRAS_CHAVE = [
    "autorregula", "autorregular", "autorregula√ß√£o", "autorregulacao", "autorreguladas",
    "manual", "guia", "c√≥digo", "codigo", "norma", "regra", "regras",
    "regula√ß√£o", "regulacao", "supervis√£o", "supervisao",
    "ades√£o", "adesao", "of√≠cio", "oficio", "circular", "penalidade", "procedimento"
]

# Garante que a pasta de sa√≠da exista
os.makedirs(PASTA_SAIDA, exist_ok=True)

def limpar_nome_arquivo(titulo):
    return re.sub(r"[^\w\s-]", "", titulo).strip().replace(" ", "_") + ".txt"

def contem_palavra_chave(texto):
    texto = texto.lower()
    return any(p in texto for p in PALAVRAS_CHAVE)

def extrair_texto_da_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        for tag in soup(["script", "style", "header", "footer", "nav"]):
            tag.decompose()

        return soup.get_text(separator="\n", strip=True)
    except Exception as e:
        print(f"‚ùå Erro ao acessar {url}: {e}")
        return None

def baixar_documentos():
    print("üîÑ Buscando documentos da API Hermes...")

    try:
        resposta = requests.get(API_DOCUMENTOS_URL, timeout=10)
        resposta.raise_for_status()
        documentos = resposta.json()
    except Exception as e:
        print(f"‚ùå Erro ao buscar documentos: {e}")
        return

    print(f"üìÑ {len(documentos)} documentos encontrados.\n")

    for doc in documentos:
        titulo = doc.get("titulo", "documento")
        link_raw = doc.get("link")

        if not link_raw or link_raw.startswith("#") or "mailto:" in link_raw:
            continue

        if "zendesk" in link_raw or "partiuinvestir" in link_raw:
            continue

        url = urljoin(BASE_SITE_ANBIMA, link_raw)

        if not contem_palavra_chave(titulo) and not contem_palavra_chave(url):
            print(f"‚è© Ignorado (irrelevante): {titulo}")
            continue

        nome_arquivo = limpar_nome_arquivo(titulo)
        caminho = os.path.join(PASTA_SAIDA, nome_arquivo)

        print(f"‚¨áÔ∏è  Baixando: {titulo}")
        texto = extrair_texto_da_url(url)

        if texto:
            with open(caminho, "w", encoding="utf-8") as f:
                f.write(texto)
            print(f"‚úÖ Salvo: {caminho}")
        else:
            print(f"‚ö†Ô∏è Falhou ao extrair: {url}")

    print("\nüèÅ Coleta conclu√≠da com sucesso!")

if __name__ == "__main__":
    baixar_documentos()
